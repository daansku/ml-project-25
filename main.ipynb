{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93084734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d10878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        decline coal start started natural gas took st...\n",
       "1        hillary clinton agrees john mccain voting give...\n",
       "2        health care reform legislation likely mandate ...\n",
       "3                     economic turnaround started end term\n",
       "4        chicago bear starting quarterback last 10 year...\n",
       "                               ...                        \n",
       "10234    larger number shark attack florida case voter ...\n",
       "10235       democrat become party atlanta metro area black\n",
       "10236    say alternative social security operates galve...\n",
       "10237           lifting cuban embargo allowing travel cuba\n",
       "10238    department veteran affair manual telling veter...\n",
       "Name: Cleaned text, Length: 10239, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and clean it\n",
    "\n",
    "df = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "\n",
    "df.drop(columns=[\"2635.json\", \"dwayne-bohac\", \"State representative\", \"Texas\", \"republican\", \"a mailer\"], inplace=True)\n",
    "df.rename(columns={\"\": \"Index\",\n",
    "                    \"false\": \"correctness\",\n",
    "                    \"Says the Annies List political group supports third-trimester abortions on demand.\": \"Text\",\n",
    "                    \"abortion\": \"Theme\",\n",
    "                    \"0\": \"barely true counts\",\n",
    "                    \"1\": \"false counts\",\n",
    "                    \"0.1\": \"half true counts\",\n",
    "                    \"0.2\": \"mostly true counts\",\n",
    "                    \"0.3\": \"pants on fire counts\",}, inplace=True)\n",
    "\n",
    "# download necessary NLTK resources (only once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "# set of stop words to filter out\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# clean text from stop words and lemmatize\n",
    "df[\"Cleaned text\"] = df[\"Text\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        [word for word in word_tokenize(str(x).lower()) if word.isalnum() and word not in stop_words]\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"Cleaned text\"] = df[\"Cleaned text\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        [lemmatizer.lemmatize(w) for w in word_tokenize(str(x).lower()) if w.isalnum()]\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"Cleaned text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f94b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m corpus = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mCleaned text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m corpus\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = df[\"Cleaned text\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8be63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
