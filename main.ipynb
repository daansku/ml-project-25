{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6bf8ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import imutils\n",
        "\n",
        "# Preprocess images, code from github https://github.com/masoudnick/Brain-Tumor-MRI-Classification/blob/main/Preprocessing.py\n",
        "\n",
        "# This cell is meant to be run only once to preprocess the images and save them in a new folder\n",
        "# You only need to run this cell once, after the folder has been created, you can ignore it\n",
        "\n",
        "def crop_img(img):\n",
        "\t\"\"\"\n",
        "\tFinds the extreme points on the image and crops the rectangular out of them\n",
        "\t\"\"\"\n",
        "\tgray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\tgray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "\n",
        "\t# threshold the image, then perform a series of erosions +\n",
        "\t# dilations to remove any small regions of noise\n",
        "\tthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "\tthresh = cv2.erode(thresh, None, iterations=2)\n",
        "\tthresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "\t# find contours in thresholded image, then grab the largest one\n",
        "\tcnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\tcnts = imutils.grab_contours(cnts)\n",
        "\tc = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "\t# find the extreme points\n",
        "\textLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "\textRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "\textTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "\textBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\tADD_PIXELS = 0\n",
        "\tnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "\t\n",
        "\treturn new_img\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c25af71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is meant to be run only once to preprocess the images and save them in a new folder\n",
        "# You only need to run this cell once, after the folder has been created, you can ignore it\n",
        "\n",
        "training = \"Training\"\n",
        "testing = \"Testing\"\n",
        "training_dir = os.listdir(training)\n",
        "testing_dir = os.listdir(testing)\n",
        "\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# create training dirs\n",
        "for dir in training_dir:\n",
        "    save_path = 'cleaned/Training/'+ dir\n",
        "    path = os.path.join(training,dir)\n",
        "    image_dir = os.listdir(path)\n",
        "    for img in image_dir:\n",
        "        image = cv2.imread(os.path.join(path,img))\n",
        "        new_img = crop_img(image)\n",
        "        new_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        cv2.imwrite(save_path+'/'+img, new_img)\n",
        "\n",
        "# create testing dirs\n",
        "for dir in testing_dir:\n",
        "    save_path = 'cleaned/Testing/'+ dir\n",
        "    path = os.path.join(testing,dir)\n",
        "    image_dir = os.listdir(path)\n",
        "    for img in image_dir:\n",
        "        image = cv2.imread(os.path.join(path,img))\n",
        "        new_img = crop_img(image)\n",
        "        new_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        cv2.imwrite(save_path+'/'+img, new_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d05ddf",
      "metadata": {},
      "source": [
        "## Training and testing with different methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e83cce5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.svm import SVC\n",
        "import mnist\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 64\n",
        "\n",
        "def super_simple_load(data_dir, img_size=(IMG_SIZE, IMG_SIZE)):\n",
        "    X, y = [], []\n",
        "    \n",
        "    for folder in os.listdir(data_dir):\n",
        "        folder_path = os.path.join(data_dir, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "            \n",
        "        # each label is assigned a numeric value\n",
        "        label = len(set(y))\n",
        "        \n",
        "        for img_file in os.listdir(folder_path):\n",
        "            if img_file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                img_path = os.path.join(folder_path, img_file)\n",
        "                img = cv2.imread(img_path, 0)  # Grayscale\n",
        "                img = cv2.resize(img, img_size).flatten() / 255.0\n",
        "                \n",
        "                X.append(img)\n",
        "                y.append(label)\n",
        "    \n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6600daef",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the data and split them into train, validation and test sets\n",
        "X_train_full, y_train_full = super_simple_load(\"training/\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, \n",
        "    test_size=0.2, \n",
        "    random_state=20,\n",
        "    stratify=y_train_full\n",
        ")\n",
        "X_test, y_test = super_simple_load(\"testing/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "083ef63c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b807ffac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize class distribution\n",
        "classes = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=y_train_full)\n",
        "plt.title('Class distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of images')\n",
        "plt.xticks(range(len(classes)), classes, rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ccbe10a",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15cdaba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get probability predictions instead of class predictions for log_loss\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "train_accuracy = model.score(X_train_full, y_train_full)\n",
        "val_accuracy = model.score(X_val, y_val)\n",
        "test_accuracy = model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Log loss: {loss}\")\n",
        "print(f\"Training accuracy: {train_accuracy}\")\n",
        "print(f\"Validation accuracy: {val_accuracy}\")\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "# Also create confusion matrix for visualization\n",
        "y_pred = model.predict(X_test)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=[\"Glioma\", \"Meningioma\", \"No tumor\", \"Pituitary\"])\n",
        "disp.plot()\n",
        "plt.title('Logistic Regression Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d37a053",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train model\n",
        "svc_model = SVC(kernel=\"linear\").fit(X_train, y_train)\n",
        "svc_y_pred = svc_model.predict(X_test)\n",
        "print(f\"SVC accuracy: {svc_model.score(X_test, y_test)}\")\n",
        "\n",
        "# confusion matrix\n",
        "svc_cm = confusion_matrix(y_test, svc_y_pred)\n",
        "svc_disp = ConfusionMatrixDisplay(confusion_matrix=svc_cm, display_labels=[\"Glioma\", \"Meningioma\", \"No tumor\", \"Pituitary\"])\n",
        "svc_disp.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdff4033",
      "metadata": {},
      "source": [
        "### CNN with keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542b42f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# hyperparameters\n",
        "num_filters = 50\n",
        "filter_size = (3, 3)\n",
        "pool_size = (2, 2)\n",
        "\n",
        "X_train_new = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "X_val_new = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(num_filters, filter_size, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    MaxPooling2D(pool_size=pool_size),\n",
        "    Flatten(),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_new,\n",
        "    to_categorical(y_train, num_classes=4),\n",
        "    epochs=15,\n",
        "    validation_data=(X_val_new, to_categorical(y_val, num_classes=4))\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train loss')\n",
        "plt.plot(history.history['val_loss'], label='Val loss')\n",
        "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
